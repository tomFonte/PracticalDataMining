---
title: "WriteUp"
author: "Tomas Fontecilla"
date: "Sunday, October 26, 2014"
output: html_document
---

This is the submission for the Data Science certification, Practical Data Mining course.

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 
```{r,echo=FALSE,results='hide'}
setwd('C:/Users/Tomas/Desktop/Data Mining/Project Writeup')
```
After setting the work directory, first step should be to download the data:

* train data url: `fileUrl = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'`
* test data url: `fileUrlTest = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'`
* download train data: `download.file(fileUrl,destfile='ProjectData.csv')`
* download test data:`download.file(fileUrlTest,destfile='ProjectTestData.csv')`

```{r=downloadData,echo=TRUE}

trainData=read.table('ProjectData.csv',header=TRUE,sep=',',na.strings=c('','NA','NULL','#DIV/0!'))
testData=read.csv('ProjectTestData.csv',header=TRUE,sep=',',na.strings=c('','NA','NULL','#DIV/0!'))
```

Taking the `na.strings=c('','NA','NULL','#DIV/0!')` ensures that this four types of strings are registered for our analysis as Non Available data.

The libraries we are going to use for this project needs to be loaded, so:
```{r LoadingLibraries,echo=TRUE}
library(caret)
library(corrplot)
```

## Cleaning the Data:

considering that most of the columns in this dataset are summaries of what we need (mostly averages, minimum, maximum, variances, standard deviations among others), and several of them are mostly unpopulated, we are considering only those columns which:

* They have 90% or more populated
* They are not a summary column

As such, we get:

```{r CleaningData,echo=TRUE}
dim(trainData)
trainData<-trainData[,colSums((is.na(trainData)))<0.9*nrow(trainData)]

nonValueAddColumns = c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window', 'X', 'user_name')
trainData<-trainData[,-which(names(trainData)%in%nonValueAddColumns)]
dim(trainData)
```

We only are going to work with 53 columns.


## Reproducibility

setting a seed to 134 we ensure the reproducibility of the analysis to be made. To determine the accuracy of a type of analysis, we are going to partition the trainData into two subsets, one with which we are going to train the algorithm, the second with which we are going to evaluate it.

```{r createPartition,echo=TRUE}
set.seed(134)
trainId = createDataPartition(trainData$classe,p=0.8,list=FALSE)
trainingData <-trainData[trainId,]
validation<-trainData[-trainId,]
```

## Analysis

The following graph shows the correlation matrix of the data to be analized
```{r CorrPlot,echo=TRUE}
correlationData <- cor(subset(trainData, select=-c(classe)))
corrplot(correlationData, order="hclust", tl.cex=0.5, method="color", insig = "pch", addrect = 10)
```

We are going to test three methods of data mining, Linear Discriminat Analysis, Random Forest and Naive Bayes. From these, we'll select the one that has the best accuracy prediction to use for the test Data classification.

```{r models,echo=TRUE}
#linear discriminant analysis
names(trainData)
ldaModel<-train(classe~.,method='lda',data=trainData)
rfModel<-train(classe~.,method='rf',data=trainData)
nbModel<-train(classe~., method='nb',data=trainData)

ldaPred<-predict(ldaModel,newdata=validation)
rfPred<-predict(rfModel,newdata=validation)
nbPred<-predict(nbModel,newdata=validation)
```

The confusion Matrices for these models are:

```{r confussionMatrices,echo=TRUE}
confusionMatrix(ldaPred,validation$classe)
confusionMatrix(rfPred,validation$classe)
confusionMatrix(nbPred,validation$classe)
```

The model with the best accuracy is the generated by Random Forest.
```{r accuracyMeasurement,echo=TRUE}
accuracy_lda<-c(as.numeric(predict(ldaModel,newdata=validation[,-ncol(validation)])==validation$classe))
accuracy<-sum(accuracy_lda)*100/nrow(validation)
print(accuracy_lda)

accuracy_rf<-c(as.numeric(predict(rfModel,newdata=validation[,-ncol(validation)])==validation$classe))
accuracy<-sum(accuracy_rf)*100/nrow(validation)
print(accuracy_rf)

accuracy_nb<-c(as.numeric(predict(nbModel,newdata=validation[,-ncol(validation)])==validation$classe))
accuracy<-sum(accuracy_nb)*100/nrow(validation)
print(accuracy_nb)
```

## Prediction

As the model with better accuracy is the Random Forest classification model, this is the one we are going to use.

```{r prediction,echo=TRUE}
testData<-testData[,-1]
predictions<-predict(rfModel,newdata=testData)
print(predictions)
```

Been this the values we are looking for, we need to write them to complete the project

```{r writeResults,echo=TRUE}
pml_write_files = function(x){
  n = length(x)
  if(!file.exists('answers')){
    dir.create('answers')
  }
  for(i in 1:n){
    filename = paste0("./answers/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictions)
```